<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Stan Notes</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
</head>
<body>
<h1 id="resources">Resources</h1>
<ul>
<li><a href="https://people.bath.ac.uk/jjf23/stan/">STAN for linear mixed models</a> by Julian Faraway, especially the penicillin example</li>
<li><a href="https://link.springer.com/article/10.3758/s13428-016-0746-9">Bayesian inference with Stan: A tutorial on adding custom distributions</a> by J Annis, B Miller, T Palmeri</li>
<li><a href="https://github.com/kholsinger/mixed-models">stan_lmer vs hard-coded Stan</a></li>
<li><a href="https://mc-stan.org/rstan/articles/rstan.html">RStan: the R interface to Stan</a></li>
<li><a href="https://kevinstadler.github.io/blog/bayesian-ordinal-regression-with-random-effects-using-brms/">Multi-level ordinal regression models with brms</a></li>
<li><a href="https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html">Estimating generalized (non-)linear models with group-specific terms with rstanarm</a></li>
</ul>
<h1 id="inner-workings-of-the-bayesian-proportional-odds-model">Inner Workings of the Bayesian Proportional Odds Model</h1>
<ul>
<li>From Nathan James: Automatic imposition of order constraint for intercepts
<ul>
<li>For our Bayesian model the intuition behind the intercept ordering is that we first estimate the probabilities of being in each of the n categories (combining observed data with the Dirichlet prior) and then apply the link function to the cumulative probabilities to calculate the intercepts. Because the cumulative probabilities are ordered and the link is monotonic, the intercepts are also ordered. Here’s a few lines of example R code that illustrate the process</li>
<li>estimated posterior prob. of membership in 7 categories (when X*beta=0)</li>
<li>est_probs &lt;- c(0.11, 0.27, 0.16, 0.14, 0.04, 0.13, 0.15)</li>
<li>estimated posterior cumulative probs</li>
<li>cum_probs &lt;- cumsum(est_probs)</li>
<li>apply logit link to cum_probs to estimate intercepts</li>
<li>est_intercepts &lt;- qlogis(cum_probs)</li>
</ul></li>
<li>FH question: Since in the limit with no ties at all the first intercept when betas are all zero or covariates are all at their mean perhaps, is first intercept is logit((n-1)/n) and the last is login(1/n), would U(logit(n-1)/n), logit(1/n)) be appropriate? This is for the model form P(Y &gt;= y). One complication in that way of thinking is if there are clustered observations the probability would perhaps not use n but would use the number of clusters (?).</li>
<li>I previously attempted to use some uniform priors for the intercepts without much success. I think there are a few problems with using U(logit((n-1)/n), logit(1/n)). First these are not necessarily bounds for the posterior <em>distributions</em> of the first and last intercept; these are values of the MLEs. Also, if you use an independent U(logit((n-1)/n), logit(1/n)) prior for each intercept, there is nothing in the form of the prior to enforce the ordering constraint. We can put an order constraint on using Stan but this is pretty inefficient (this is the same reason why using brms for ordinal regression is inefficient with many categories). Both of these lead to sampling issues since we have so little data to inform the location of the intercepts in the continuous case. Last, if we’re truly in a continuous data setting, we don’t know the value of n before collecting the data, so we wouldn’t be able define the limits of the uniform distribution. To me, using the observed data to define the prior (even in this somewhat limited way) is a red flag that we’re “double-dipping”. We should be able to define the prior without seeing any data. That’s what I’m trying to do with the Dirichlet process prior.</li>
<li>Ben Goodrich reply: Dirichlet process makes sense conceptually, although you can’t do it literally in Stan because it concentrates on a finite set. Some people have had some success with a giant but finite number of components in a Dirichlet. Also, this reminds me of some stuff that other people were doing to use a spline or similar function for the baseline hazard in survival models, which also has to be strictly increasing.</li>
<li>Nathan: Ben - I agree with you about implementing a ‘true’ Dirichlet process in Stan; I think it will be difficult using HMC even with a finite Dirichlet process so I may have to venture outside the Stan-verse. As you mentioned there are close ties to other semi-parametric models such as those used in survival analysis. Frank - Can you elaborate more on the conditioning on sample size? In many models we assume n is fixed rather than a parameter, but the value of n doesn’t play any part in defining the form or number of parameters. Shouldn’t we be able to make draws from the prior distribution without knowing the sample size? It seems that assuming knowledge about the number of outcome categories for a traditional ordinal regression is qualitatively different than assuming we know the number of distinct continuous outcome values, but I’m not sure I can explain why.</li>
<li>Frank: The comment about a prior needing to have a kind of universal meaning is very interesting.  The prior should capture the pre-data state of knowledge, and if we have knowledge that uses n (which we’re already conditioning on) but does not use the data in any other way, perhaps this is OK.</li>
</ul>
<h2 id="existing-stan-approaches">Existing <code>Stan</code> Approaches</h2>
<ul>
<li>See <a href="ttps://mc-stan.org/docs/2_23/stan-users-guide/ordered-logistic-section.html">this</a></li>
<li>Nathan: The model they show is the Stan manual uses improper priors for the cutpoints with only the ‘ordered’ constraint in the parameter block to make sure they are increasing. The order constraint itself converts to an unconstrained space using a log difference transformation, i.e. delta_1 = alpha_1, delta_2 = log(alpha_2 - alpha_1), delta_3 = log(alpha_3 - alpha_2). If the prior on the constrained space is improper, then the unconstrained prior will also be improper so a proper posterior distribution is not guaranteed. Usually, if there is a reasonable amount of observed data for each category then you can get convergence, but if the amount of data for each category is low (e.g. continuous CPM case or traditional ordinal model with low or zero counts in one or more categories) there can be problems getting the model to converge.</li>
<li>Michael Betancourt has a good blog post that goes into more detail and also describes the alternative Dirichlet prior <a href="https://betanalpha.github.io/assets/case_studies/ordinal_regression.html">here</a></li>
</ul>
<h1 id="qr-decomposition-for-orthonormalization">QR Decomposition for Orthonormalization</h1>
<ul>
<li>From Ben Goodrich: The last <span class="math inline">\(\beta\)</span> parameter is unchanged so its <span class="math inline">\(\beta\)</span> equals its <span class="math inline">\(\theta\)</span> parameter in the Stan program. This will sometimes create a warning message (e.g. when using <code>pairs()</code> that “beta[…] is duplicative”).</li>
</ul>
<h1 id="general-information-about-random-effects">General Information About Random Effects</h1>
<ul>
<li>From Jonathan Schildcrout: Omitting random effects from a model
<ul>
<li>This is not bias. The two models are estimating different parameters. For Binary data there are approximations that can be used to convert between the marginal (what LRM estimates) and conditional (what random effects estimate) parameters. See Zeger Liang and Albert 1988. Not sure if that works for general ordinal data.</li>
<li>The ZLA conversion is this</li>
<li>Sigma = 2.04</li>
<li>C = 16<em>sqrt(3)/(15</em>3.14)</li>
<li>Mult = 1 / sqrt(c^2 * Sigma^2 +1) = 0.76</li>
<li>beta.marg = Mult * beta.cond</li>
<li>You can think of the sd of the random effects as a coefficient for a standardized covariate that was not observed: <span class="math inline">\(Logit(p_{ij}) = x_{ij} \beta + b_i\)</span>, and <span class="math inline">\(b_i ~ N(0, \sigma^2)\)</span> which means <span class="math inline">\(Logit(p_{ij}) = x_{ij} \beta + \sigma * Z_i\)</span> , and <span class="math inline">\(Z_i ~ N(0, 1)\)</span></li>
</ul></li>
<li>Ben Goodrich: Why not use a prior that is n(0, <span class="math inline">\(\sigma_{\gamma}\)</span>)
<ul>
<li>The main thing is that usually (but unfortunately not always) the MCMC goes better if you utilize the stochastic representation of the normal distribution, which entails putting the unscaled random effects in the parameters block and scaling them by sigmag in the transformed parameters block to get the scaled random effects that go into the likelihood function. Then, the unscaled random effects get a standard normal prior, which implies before you see the data that the scaled random effects are distributed normal with mean zero and standard deviation sigmag. That tends to reduce the posterior dependence between sigmag and the (unscaled) random effects. In addition, there should be a proper prior on sigmag. I put it as exponential, but you would need to pass in the prior rate from R.</li>
</ul></li>
</ul>
<h1 id="mles-and-optimization-for-random-effects-models">MLEs and Optimization for Random Effects Models</h1>
<ul>
<li>From Ben Goodrich: Inconsistent results when using <code>rstan::optimizing</code>
<ul>
<li>Optimizing is not going to work well in hierarchical models that condition on the values of the random effects because (penalized) MLE is not a consistent estimator of (any of) the parameters as the number of clusters gets large. That is why Frequentists have to first integrate the random effects out of the likelihood before they can choose the fixed effects to maximize it. If you do MCMC, then the posterior means and medians should be much more stable than the modes. You can also try the vb() function, which implements variational Bayesian inference — essentially trying to find the closest multivariate normal distribution to posterior distribution in the unconstrained space — but that tends to not work well in general either.</li>
</ul></li>
</ul>
<h1 id="prediction-with-random-effects">Prediction With Random Effects</h1>
<ul>
<li>Ben Goodrich: Everything about the random effect stuff is easier from a Bayesian perspective, except for one thing: If you want to evaluate the model based on how it is expected to predict new patients (that by definition have not been observed yet), then you have to re-calculate the log-likelihood contributions in generated quantities after numerically integrating out the random effects like the Frequentists do. This is not so bad to code now that Stan has a one-dimensional numerical integration function, but it takes doing. See https://arxiv.org/abs/1802.04452</li>
</ul>
<h1 id="ar1-modeling">AR(1) Modeling</h1>
<ul>
<li><a href="https://discourse.mc-stan.org/t/migrated-from-google-group-ar-1-logistic-regression-funnel" class="uri">https://discourse.mc-stan.org/t/migrated-from-google-group-ar-1-logistic-regression-funnel</a></li>
<li><a href="https://discourse.mc-stan.org/t/improving-efficiency-when-modeling-autocorrelation" class="uri">https://discourse.mc-stan.org/t/improving-efficiency-when-modeling-autocorrelation</a></li>
<li><a href="https://discourse.mc-stan.org/t/dynamic-panel-data-models-with-stan" class="uri">https://discourse.mc-stan.org/t/dynamic-panel-data-models-with-stan</a></li>
<li><a href="https://github.com/jgabry/stancon2018helsinki_intro/tree/master/slides" class="uri">https://github.com/jgabry/stancon2018helsinki_intro/tree/master/slides</a></li>
<li><a href="https://github.com/jgabry/stancon2018helsinki_intro/blob/master/Pest_Control_Example.Rmd" class="uri">https://github.com/jgabry/stancon2018helsinki_intro/blob/master/Pest_Control_Example.Rmd</a></li>
<li><a href="https://mc-stan.org/docs/2_20/stan-users-guide/autoregressive-section.html" class="uri">https://mc-stan.org/docs/2_20/stan-users-guide/autoregressive-section.html</a></li>
<li><a href="https://www.mathworks.com/help/econ/simulate-stationary-arma-processes.html" class="uri">https://www.mathworks.com/help/econ/simulate-stationary-arma-processes.html</a> (kick starting the process with unconditional mean)</li>
<li><a href="https://www.math.utah.edu/~zhorvath/ar1.pdf" class="uri">https://www.math.utah.edu/~zhorvath/ar1.pdf</a> (recursive substitution p. 5)</li>
</ul>
<p>The equation in the last reference p. 5 allows specification of the random effect at time t without passing through all the previous random effects, so it accounts for unequally spaced and missing time points. It suggests this model:</p>
<ul>
<li>Let <span class="math inline">\(\gamma_i\)</span> be a <span class="math inline">\(n(0, \sigma_\gamma\)</span>) random effect for the <span class="math inline">\(i\)</span>th subject.</li>
<li>Let <span class="math inline">\(\epsilon_1, ... \epsilon_T\)</span> be the within-subject white noise that is <span class="math inline">\(n(0, \sigma_w)\)</span>, where <span class="math inline">\(T\)</span> is the maximum follow-up time (we may only use the first few of these for a given subject)</li>
<li>Then the random effect for subject <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span> is <span class="math inline">\(r_{i,t} = \rho^{t}\gamma_i + \rho^{t-1}\epsilon_1 + \rho^{t-2}\epsilon_2 + ... \epsilon_k\)</span></li>
<li>Or since the white noise <span class="math inline">\(\epsilon\)</span> are generated while Stan is running, they will all be defined regardless of which observations are actually observed, so the standard specification should work: <span class="math inline">\(r_{i,1} = \gamma_i, r_{i,2} = \rho r_{i,1} + \epsilon_2, r_{i,3} = \rho r_{i, 2} + \epsilon_3, ...\)</span>.</li>
</ul>
<p>Would this specification lead to sampling problems? Do <span class="math inline">\(\sigma_\gamma\)</span> and <span class="math inline">\(\sigma_w\)</span> compete too much?</p>
<p>Ben Goodrich re warnings about Pareto k diagnostics from <code>loo()</code>: The <code>loo()</code> function is trying to estimate what would happen if 1 patient were dropped from the analysis and predicted conditional on all the other patients. But its importance sampling has infinite variance when the Pareto k for the left-out observation is greater than 1 and has too high variance if the Pareto k is greater than 0.7. The Pareto k pertains to how much the posterior distribution would change if one observation were left out. In these models, if one person were left out the corresponding gamma and that patient’s column of eps_raw would revert to their prior distributions because there would no longer be any information in the data to update them with. Thus, this posterior distribution is too sensitive to the particular patients being conditioned on to estimate the expected log predictive density of future patients well.</p>
<p>To overcome this problem, we can do K-fold with K equal to the number of patients or redo the loo calculation to use a likelihood function that integrates the random effects out like in a Frequentist estimator, as described in that Psychometrica article I sent you the link to. But we can still use Stan to first obtain the posterior draws conditional on the random effects.</p>
<p>FH question on homescedasticity of random effects: With random effect for subject <span class="math inline">\(i\)</span> at the first time <span class="math inline">\(t=1\)</span> having variance <span class="math inline">\(\sigma^2_\gamma\)</span>, we can use the recursive relationship <span class="math inline">\(V(X_t) = \rho^2 V(X_{t-1}) + \sigma^2_\epsilon\)</span> to get variances at other times, where <span class="math inline">\(\sigma^2_\epsilon\)</span> is the within-subject white noise variance. The variance of the random effect at time <span class="math inline">\(t=2\)</span> is <span class="math inline">\(\rho^2 \sigma^2_\gamma + \sigma^2_\epsilon\)</span> and equating the two successive variances results in <span class="math inline">\(\sigma_\epsilon = \sigma_\gamma \sqrt{1 - \rho^2}\)</span>. The same equation results from equating the variance at <span class="math inline">\(t=3\)</span> to the variance at <span class="math inline">\(t=2\)</span>. So it is reasonable to not make <span class="math inline">\(\sigma_\epsilon\)</span> a free parameter but instead to derive it from <span class="math inline">\(\sigma_\gamma\)</span> and <span class="math inline">\(\rho\)</span>? Would this make posterior sampling behave much better too?</p>
<p><a name="ppo"></a></p>
<h1 id="partial-proportional-odds-model">Partial Proportional Odds Model</h1>
<p>The <a href="http://hbiostat.org/papers/feh/pet90par.pdf">PPO model</a> of Peterson and Harrell (1990) in its <em>unconstrained</em> form (Eq. (5) of the reference) has this specification for a single observation when <span class="math inline">\(Y=1, 2, ..., k\)</span> when <span class="math inline">\(j &gt; 1\)</span> (the paper uses a different coding, for <span class="math inline">\(Y=0, ..., k\)</span> so their <span class="math inline">\(k\)</span> is our <span class="math inline">\(k-1\)</span>)):</p>
<p><span class="math display">\[P(Y \geq j | X) = \text{expit}(\alpha_{j-1} + X\beta + [j &gt; 2] Z\tau_{j-2}) = \text{expit}(c_{j})\]</span></p>
<ul>
<li><span class="math inline">\(c_{1}\)</span>: undefined and unused</li>
<li><span class="math inline">\(\alpha\)</span>: <span class="math inline">\(k-1\)</span> vector of overall intercepts</li>
<li><span class="math inline">\(\tau\)</span>: <span class="math inline">\(k-2 \times q\)</span> matrix of parameters</li>
</ul>
<p>For the entire dataset <span class="math inline">\(Z\)</span> is an <span class="math inline">\(n\times q\)</span> design matrix specifying the form of departures from proportional odds, and to honor the hierarchy principle for interactions must be a subset of the columns of <span class="math inline">\(X_{n, p}\)</span>. With regard to <span class="math inline">\(Z\)</span> the model is multinomial instead of ordinal, and so unlike the PO model there are issues with cell sizes in the <span class="math inline">\(Y\)</span> frequency distribution. The unconstrained PPO model is strictly for discrete ordinal <span class="math inline">\(Y\)</span> and there must be at least <span class="math inline">\(k=3\)</span> levels of <span class="math inline">\(Y\)</span>. The <span class="math inline">\(\alpha\)</span>s are the intercepts for <span class="math inline">\(Y \geq 2\)</span> (and thus their negatives are intercepts for <span class="math inline">\(Y=1\)</span>).</p>
<p>Likelihood components are as follows:</p>
<ul>
<li><span class="math inline">\(Y=1\)</span>: <span class="math inline">\(\text{expit}(- c_2)\)</span> (<span class="math inline">\(Z\)</span> ignored)</li>
<li><span class="math inline">\(Y=2, ..., k-1\)</span>: <span class="math inline">\(\text{expit}(c_Y) - \text{expit}(c_{Y+1})\)</span> (<span class="math inline">\(Z\)</span> ignored in first term when <span class="math inline">\(Y=2\)</span>)</li>
<li><span class="math inline">\(Y=k\)</span>: <span class="math inline">\(\text{expit}(c_k)\)</span> (<span class="math inline">\(Z\)</span> used)</li>
</ul>
<p>In <code>Stan</code> code <code>prmqrcppo.stan</code>, <span class="math inline">\(Z\)</span> is orthonormalized by the QR decomposition, and the PPO parameters on this new scale are <span class="math inline">\(\omega\)</span> instead of <span class="math inline">\(\tau\)</span> just as <span class="math inline">\(\theta\)</span> is substituted for <span class="math inline">\(\beta\)</span>. This code implements cluster (random) effects, so if there are no repeated observations per subject the user needs to specify a very small mean for the exponential prior for the variance of random effects (e.g., 0.001).</p>
<p>The <span class="math inline">\(p\)</span>-vector of normal prior standard deviations for <span class="math inline">\(\theta\)</span> is <code>sds</code> and a separate <span class="math inline">\(k-2 \times q\)</span> matrix of normal prior SDs for <span class="math inline">\(\omega\)</span> is given by <code>sdsppo</code>. If a binary treatment variable is present and one wants full control over its prior SD, be sure to put treatment as the last parameter for <span class="math inline">\(X\)</span>, and if treatment is allowed to violate the PO assumption also put treatment as the last parameter for <span class="math inline">\(Z\)</span>.</p>
<p>To some extent the constrained PPO model may be obtained by using very skeptical priors on the <span class="math inline">\(\omega\)</span> (transformed <span class="math inline">\(\tau\)</span>) parameters, e.g., standard deviations &lt; 1.0. This will lower the effective number of model parameters.</p>
<h1 id="pertinent-stan-documentation">Pertinent Stan Documentation</h1>
<ul>
<li><a href="https://mc-stan.org/docs/2_23/stan-users-guide/basic-motivation.html">Basics</a></li>
<li><a href="https://mc-stan.org/docs/2_23/stan-users-guide/multi-indexing-chapter.html">Multi-indexes</a></li>
<li><a href="https://mc-stan.org/docs/2_23/functions-reference/mixed-operations.html">Mixed operations</a></li>
<li><a href="https://mc-stan.org/docs/2_23/stan-users-guide/log-sum-of-exponentials.html">log Sums</a></li>
<li><a href="https://mc-stan.org/docs/2_23/stan-users-guide/QR-reparameterization-section.html">Reparameterization</a></li>
<li><a href="https://mc-stan.org/docs/2_23/stan-users-guide/ordered-logistic-section.html">Ordered logisitic</a></li>
</ul>
<h2 id="especially-relevant-functions">Especially Relevant Functions</h2>
<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Computes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>log1m_exp</td>
<td>log(1 - exp(x))</td>
</tr>
<tr class="even">
<td>log1p_exp</td>
<td>log(1 + exp(x))</td>
</tr>
<tr class="odd">
<td>log_diff_exp</td>
<td>log(exp(x) - exp(y))</td>
</tr>
<tr class="even">
<td>-log1p_exp(-x)</td>
<td>log(expit(x))</td>
</tr>
</tbody>
</table>
<p>Also study <code>bernoulli_logit</code> and <code>categorical_logit</code>.</p>
<h1 id="other-useful-links">Other Useful Links</h1>
<ul>
<li><a href="https://discourse.mc-stan.org/t/bias-in-main-effects-for-logistic-model-with-random-intercept-and-small-cluster-sizes">Logistic model with random intercept and small cluster sizes</a></li>
<li><a href="https://discourse.mc-stan.org/t/algebra-solver-has-a-side-effect-on-log-probability">Implicit parameters as with marginal structural models</a></li>
</ul>
</body>
</html>
